{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get job listings from linkedin\n",
    "def getListingsInPage(jobTitle, location, num):\n",
    "\n",
    "    #get listings for specific job title and location\n",
    "    listUrl = f\"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords={jobTitle}&location={location}&geoId=&trk=public_jobs_jobs-search-bar_search-submit&start={num}\"\n",
    "\n",
    "    response = requests.get(listUrl)\n",
    "    listData = response.text\n",
    "    listSoup = BeautifulSoup(listData, \"html.parser\")\n",
    "    pageListings = listSoup.find_all(\"li\")\n",
    "\n",
    "    return pageListings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract some info from a job listing\n",
    "def getIdTitleDateLoc(pageListings):\n",
    "#Get the job listing ID for each listing in the search\n",
    "    idTitleDateLoc = []\n",
    "    for job in pageListings:\n",
    "        baseCardDiv = job.find(\"div\",{\"class\":\"base-card\"})\n",
    "\n",
    "        jobID = baseCardDiv.get(\"data-entity-urn\").split(\":\")[3]\n",
    "\n",
    "        try:\n",
    "            jobTitle = job.find(\"h3\",{\"class\" : \"base-search-card__title\"}).text.strip()\n",
    "        except:\n",
    "            jobTitle = None\n",
    "\n",
    "        try:\n",
    "            postingDate = job.find(\"time\", {\"class\": \"job-search-card__listdate\"}).get(\"datetime\")\n",
    "        except:\n",
    "            postingDate = None\n",
    "\n",
    "        try:\n",
    "            location = job.find(\"span\", {\"class\": \"job-search-card__location\"}).text.strip()\n",
    "        except:\n",
    "            location = None\n",
    "\n",
    "        idTitleDateLoc.append([jobID,jobTitle, postingDate,location])\n",
    "    return idTitleDateLoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRequirements(soup):\n",
    "    sectionTitles = [\n",
    "        \"Minimum Requirements\", \"Qualifications\", \"Job Requirements\", \n",
    "        \"Minimum Qualifications\", \"Skills\", \"Experience Required\", \"Skills Required\", \n",
    "        \"Requirements\", \"Preferred Qualifications\", \"You will be a great fit if you have:\",\n",
    "        \"Must Have\", \"Skills and Qualifications\", 'What you bring to the table?', \n",
    "        'Required Skills', 'What we are looking for in you', 'Skills and Responsibilities:',\n",
    "        'Who You Might Be', 'Experience', 'Job Description', 'What You\\'ll Need', 'What We Look For', \n",
    "        'What you bring', 'Who you are', 'Basic Qualifications', 'Your Qualifications', \n",
    "        'Required experience and skills'\n",
    "    ]\n",
    "\n",
    "    for title in sectionTitles:\n",
    "        section = soup.find(lambda tag: tag.name in [\"strong\", \"h2\", \"h3\", \"h4\"] and title.lower() in tag.text.lower())\n",
    "        if section:\n",
    "            ul = section.find_next(\"ul\")\n",
    "            if ul:\n",
    "                return [li.text.strip() for li in ul.find_all(\"li\")]\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCriteria(soup):\n",
    "    criteria = {}\n",
    "    criteriaList = soup.find(\"ul\", {\"class\": \"description__job-criteria-list\"})\n",
    "    if criteriaList:\n",
    "        for item in criteriaList.find_all(\"li\", {\"class\": \"description__job-criteria-item\"}):\n",
    "            header = item.find(\"h3\", {\"class\": \"description__job-criteria-subheader\"})\n",
    "            value = item.find(\"span\", {\"class\": \"description__job-criteria-text description__job-criteria-text--criteria\"})\n",
    "            if header and value:\n",
    "                criteria[header.text.strip().lower()] = value.text.strip()\n",
    "    return criteria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addToJson(idTitleDateLoc, file):\n",
    "    dbfsPath = f\"/dbfs/mnt/data/{file}\"\n",
    "    jobPostings = []\n",
    "    existingIds = set()\n",
    "\n",
    "    try:\n",
    "        with open(dbfsPath, 'r') as jsonFile:\n",
    "            print(\"Added.\")\n",
    "            jobPostings = json.load(jsonFile)\n",
    "            existingIds = {job[\"jobID\"] for job in jobPostings}\n",
    "    except FileNotFoundError:\n",
    "        print(\"No existing data found. Starting fresh.\")\n",
    "    \n",
    "    for job in idTitleDateLoc:\n",
    "        if job[0] in existingIds:\n",
    "            continue\n",
    "\n",
    "        jobUrl = f\"https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{job[0]}\"\n",
    "        jobResponse = requests.get(jobUrl)\n",
    "        jobData = jobResponse.text\n",
    "        jobSoup = BeautifulSoup(jobData, \"html.parser\")\n",
    "\n",
    "        jobPost = {\n",
    "            \"jobID\": job[0],\n",
    "            \"jobTitle\": job[1],\n",
    "            \"DatePosted\": job[2],\n",
    "            \"Location\": job[3]\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            jobPost[\"companyName\"] = jobSoup.find(\"a\", {\"class\": \"topcard__org-name-link topcard__flavor--black-link\"}).text.strip()\n",
    "        except AttributeError:\n",
    "            jobPost[\"companyName\"] = None\n",
    "        \n",
    "        jobPost[\"requirements\"] = getRequirements(jobSoup)\n",
    "        jobPost.update(getCriteria(jobSoup))\n",
    "\n",
    "        jobPostings.append(jobPost)\n",
    "\n",
    "    # Write the updated job postings to the file once after processing all jobs\n",
    "    with open(dbfsPath, 'w') as jsonFile:\n",
    "        json.dump(jobPostings, jsonFile, indent=4)\n",
    "\n",
    "    return jobPostings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataForJobTitle(title, loc, end, f):\n",
    "    n = 10\n",
    "    while n != end:\n",
    "        try:\n",
    "            pageListings = getListingsInPage(title, loc, str(n))\n",
    "            details = getIdTitleDateLoc(pageListings)\n",
    "            addToJson(details, f)\n",
    "        except:\n",
    "            continue\n",
    "        n+=10\n",
    "    print(\"Data successfully added for \"+title+ ' postings in ' + loc)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
