{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadJsonFromDbfs(jsonFile):\n",
    "    with open(f\"/dbfs/mnt/data/{jsonFile}\", 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_job_postings(jobs1, jobs2):\n",
    "    job_postings = {job[\"jobID\"]: job for job in jobs1}\n",
    "    for job in jobs2:\n",
    "        if job[\"jobID\"] not in job_postings:\n",
    "            job_postings[job[\"jobID\"]] = job\n",
    "    return list(job_postings.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a CSV of the requirements where each column is one of the job requirements to ease skills extraction"
    "def createRequirementsCSV(jsonFile):\n",
    "    \n",
    "    output_csv_path = f\"/mnt/data/requirements.csv\"\n",
    "    with open('/dbfs' + output_csv_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Write the header\n",
    "        header = [\"jobID\"] + [f\"requirement_{i+1}\" for i in range(max(len(job.get(\"requirements\", [])) for job in jsonFile))]\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # Write the data\n",
    "        for job in jsonFile:\n",
    "            row = [job.get(\"jobID\", \"\")]\n",
    "            row.extend(job.get(\"requirements\", [\"\"] * (len(header) - 1)))\n",
    "            writer.writerow(row)\n",
    "        \n",
    "    print(\"Requirements CSV file has been created successfully and saved to DBFS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createJobDataCSV(data):\n",
    "    output_csv_path = f\"/mnt/data/jobData.csv\"\n",
    "    with open('/dbfs' + output_csv_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "            \n",
    "        # Collect all field names except 'requirements'\n",
    "        all_fields = set()\n",
    "        for job in data:\n",
    "            all_fields.update(job.keys())\n",
    "            all_fields.discard('requirements')\n",
    "            \n",
    "        # Write the header\n",
    "        header = list(all_fields)\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # Write the data\n",
    "        for job in data:\n",
    "            row = [job.get(field, \"\") for field in header]\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(\"JobData CSV creation completed and saved to 'jobData.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creatingTableInDatabricks():\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = spark.read.option(\"header\", \"true\").csv(\"/mnt/data/jobData.csv\")\n",
    "\n",
    "    # Rename columns to remove invalid characters\n",
    "    df_clean = df.select([col(c).alias(c.replace(' ', '_').replace(';', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\\n', '').replace('\\t', '').replace('=', '')) for c in df.columns])\n",
    "\n",
    "    # Create a table from the cleaned DataFrame\n",
    "    df_clean.write.saveAsTable(\"jobData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePostingsWithEmptyRequirements(job_postings):\n",
    "    return [job for job in job_postings if job.get(\"requirements\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCsvOfExtractedRequirements(jobs):\n",
    "\n",
    "    # Prepare the CSV content\n",
    "    csv_rows = []\n",
    "    headers = ['jobID']\n",
    "\n",
    "    # Prepare the rows and headers dynamically\n",
    "    for job in jobs:\n",
    "        job_id = job['jobID']\n",
    "        requirements = job['requirements']\n",
    "        row = [job_id] + requirements\n",
    "        csv_rows.append(row)\n",
    "        # Add requirement headers if more columns are needed\n",
    "        while len(headers) < len(row):\n",
    "            headers.append(f\"requirement{len(headers)}\")\n",
    "\n",
    "    # Write the CSV file\n",
    "    output_csv_path = f\"/mnt/data/requirements.csv\"\n",
    "    with open('/dbfs' + output_csv_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        # Write the headers\n",
    "        writer.writerow(headers)\n",
    "        # Write the rows\n",
    "        for row in csv_rows:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(\"CSV file has been created successfully and saved to DBFS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_postings_date_range(json_data):\n",
    "    # Extract the DatePosted values\n",
    "    dates = [job['DatePosted'] for job in json_data if job['DatePosted'] is not None]\n",
    "    \n",
    "    # Convert the dates to datetime objects\n",
    "    date_objects = [datetime.strptime(date, '%Y-%m-%d') for date in dates]\n",
    "    \n",
    "    # Find the minimum and maximum dates\n",
    "    min_date = min(date_objects)\n",
    "    max_date = max(date_objects)\n",
    "    \n",
    "    # Convert the datetime objects back to strings\n",
    "    min_date_str = min_date.strftime('%Y-%m-%d')\n",
    "    max_date_str = max_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    return min_date_str, max_date_str\n",
    "\n",
    "# Find the date range\n",
    "min_date, max_date = find_postings_date_range(mergedNoEmptyReq)\n",
    "print(f\"The date range of DatePosted is from {min_date} to {max_date}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_postings_per_year(json_data):\n",
    "    # Extract the years from DatePosted values\n",
    "    years = [datetime.strptime(job['DatePosted'], '%Y-%m-%d').year for job in json_data if job['DatePosted'] is not None]\n",
    "    \n",
    "    # Count the occurrences of each year\n",
    "    year_counts = Counter(years)\n",
    "    \n",
    "    return year_counts\n",
    "\n",
    "year_counts = count_postings_per_year(mergedNoEmptyReq)\n",
    "for year, count in year_counts.items():\n",
    "    print(f\"Year: {year}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_postings_per_month(json_data):\n",
    "    # Extract the year and month from DatePosted values\n",
    "    year_months = [datetime.strptime(job['DatePosted'], '%Y-%m-%d').strftime('%Y-%m') for job in json_data if job['DatePosted'] is not None]\n",
    "    \n",
    "    # Count the occurrences of each year-month\n",
    "    year_month_counts = Counter(year_months)\n",
    "    \n",
    "    return year_month_counts\n",
    "\n",
    "\n",
    "# Get the count of postings per month\n",
    "year_month_counts = count_postings_per_month(mergedNoEmptyReq)\n",
    "for year_month, count in year_month_counts.items():\n",
    "    print(f\"Year-Month: {year_month}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_postings_per_location(json_data):\n",
    "    # Extract the locations\n",
    "    locations = [job['Location'] for job in json_data]\n",
    "    \n",
    "    # Count the occurrences of each location\n",
    "    location_counts = Counter(locations)\n",
    "    \n",
    "    return location_counts\n",
    "\n",
    "# Get the count of postings per location\n",
    "location_counts = count_postings_per_location(mergedNoEmptyReq)\n",
    "print(\"Count of job postings per location:\")\n",
    "for location, count in location_counts.items():\n",
    "    print(f\"Location: {location}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_province(location):\n",
    "    # Split the location string and assume the province is the second last element\n",
    "    parts = location.split(', ')\n",
    "    if len(parts) >= 2:\n",
    "        province = parts[-2]\n",
    "        return province\n",
    "    return None\n",
    "\n",
    "def count_postings_per_province(json_data):\n",
    "    # Extract the provinces from the locations\n",
    "    provinces = [extract_province(job['Location']) for job in json_data if extract_province(job['Location']) is not None]\n",
    "    \n",
    "    # Count the occurrences of each province\n",
    "    province_counts = Counter(provinces)\n",
    "    \n",
    "    return province_counts\n",
    "\n",
    "# Get the count of postings per province\n",
    "province_counts = count_postings_per_province(mergedNoEmptyReq)\n",
    "print(\"Count of job postings per province:\")\n",
    "for province, count in province_counts.items():\n",
    "    print(f\"Province: {province}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_montreal_area(location):\n",
    "    # Define a set of keywords that indicate Montreal or Greater Montreal area\n",
    "    montreal_keywords = {\n",
    "        \"Montreal, Quebec, Canada\",\n",
    "        \"Greater Montreal Metropolitan Area\",\n",
    "        \"Greater Montreal, Quebec, Canada\",\n",
    "        \"Montréal-Ouest, Quebec, Canada\",\n",
    "        \"Montréal-Est, Quebec, Canada\",\n",
    "        \"Saint-Laurent, Quebec, Canada\",\n",
    "        \"Dorval, Quebec, Canada\",\n",
    "        \"Longueuil, Quebec, Canada\",\n",
    "        \"Brossard, Quebec, Canada\",\n",
    "        \"Vaudreuil-Dorion, Quebec, Canada\",\n",
    "        \"Laval, Quebec, Canada\"\n",
    "    }\n",
    "    return any(keyword in location for keyword in montreal_keywords)\n",
    "\n",
    "def count_montreal_area_postings(json_data):\n",
    "    # Count the occurrences of Montreal or Greater Montreal area postings\n",
    "    montreal_count = sum(1 for job in json_data if is_montreal_area(job['Location']))\n",
    "    return montreal_count\n",
    "\n",
    "def is_vancouver_area(location):\n",
    "    # Define a set of keywords that indicate Vancouver or Greater Vancouver area\n",
    "    vancouver_keywords = {\n",
    "        \"Vancouver, British Columbia, Canada\",\n",
    "        \"Greater Vancouver Metropolitan Area\",\n",
    "        \"Greater Vancouver, British Columbia, Canada\",\n",
    "        \"North Vancouver, British Columbia, Canada\",\n",
    "        \"West Vancouver, British Columbia, Canada\",\n",
    "        \"Richmond, British Columbia, Canada\",\n",
    "        \"Burnaby, British Columbia, Canada\",\n",
    "        \"Surrey, British Columbia, Canada\",\n",
    "        \"Delta, British Columbia, Canada\",\n",
    "        \"New Westminster, British Columbia, Canada\",\n",
    "        \"Langley, British Columbia, Canada\",\n",
    "        \"Port Coquitlam, British Columbia, Canada\",\n",
    "        \"Port Moody, British Columbia, Canada\",\n",
    "        \"Maple Ridge, British Columbia, Canada\",\n",
    "        \"White Rock, British Columbia, Canada\",\n",
    "        \"Greater Vancouver, British Columbia, Canada\"\n",
    "    }\n",
    "    return any(keyword in location for keyword in vancouver_keywords)\n",
    "\n",
    "def is_toronto_area(location):\n",
    "    # Define a set of keywords that indicate Toronto or Greater Toronto area\n",
    "    toronto_keywords = {\n",
    "        \"Toronto, Ontario, Canada\",\n",
    "        \"Greater Toronto Area, Canada\",\n",
    "        \"North York, Ontario, Canada\",\n",
    "        \"East York, Ontario, Canada\",\n",
    "        \"Scarborough, Ontario, Canada\",\n",
    "        \"Etobicoke, Ontario, Canada\",\n",
    "        \"York, Ontario, Canada\",\n",
    "        \"Mississauga, Ontario, Canada\",\n",
    "        \"Brampton, Ontario, Canada\",\n",
    "        \"Vaughan, Ontario, Canada\",\n",
    "        \"Markham, Ontario, Canada\",\n",
    "        \"Richmond Hill, Ontario, Canada\",\n",
    "        \"Oakville, Ontario, Canada\",\n",
    "        \"Burlington, Ontario, Canada\"\n",
    "    }\n",
    "    return any(keyword in location for keyword in toronto_keywords)\n",
    "\n",
    "def count_area_postings(json_data, area_check_function):\n",
    "    # Count the occurrences of postings in the specified area\n",
    "    area_count = sum(1 for job in json_data if area_check_function(job['Location']))\n",
    "    return area_count"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
